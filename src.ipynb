{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## importing the libraries"
      ],
      "metadata": {
        "id": "p9ECNAtDuPgI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2Hgcf_phPve"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import math\n",
        "# import os\n",
        "import PIL\n",
        "# for reading and displaying images\n",
        "# from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "# for creating validation set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# for evaluating the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# PyTorch libraries and modules\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout, init,Sigmoid\n",
        "from torch.optim import Adam, SGD\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unrar x \"/content/drive/MyDrive/Dataset (1).rar\" \"/content/el_7aga/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgOG--gJErgq",
        "outputId": "f15021f9-d612-4a33-e57a-40a5b611acab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/drive/MyDrive/Dataset (1).rar\n",
            "\n",
            "\n",
            "Would you like to replace the existing file /content/el_7aga/DLCV_data.mat\n",
            "8110081736 bytes, modified on 2022-06-02 10:49\n",
            "with a new one\n",
            "8110081736 bytes, modified on 2022-06-02 10:49\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  /content/el_7aga/DLCV_data.mat                               \b\b\b\b  0%\b\b\b\b  1%\b\b\b\b  2%\b\b\b\b  3%\b\b\b\b  4%\b\b\b\b  5%\b\b\b\b  6%\b\b\b\b  7%\b\b\b\b  8%\b\b\b\b  9%\b\b\b\b 10%\b\b\b\b 11%\b\b\b\b 12%\b\b\b\b 13%\b\b\b\b 14%\b\b\b\b 15%\b\b\b\b 16%\b\b\b\b 17%\b\b\b\b 18%\b\b\b\b 19%\b\b\b\b 20%\b\b\b\b 21%\b\b\b\b 22%\b\b\b\b 23%\b\b\b\b 24%\b\b\b\b 25%\b\b\b\b 26%\b\b\b\b 27%\b\b\b\b 28%\b\b\b\b 29%\b\b\b\b 30%\b\b\b\b 31%\b\b\b\b 32%\b\b\b\b 33%\b\b\b\b 34%\b\b\b\b 35%\b\b\b\b 36%\b\b\b\b 37%\b\b\b\b 38%\b\b\b\b 39%\b\b\b\b 40%\b\b\b\b 41%\b\b\b\b 42%\b\b\b\b 43%\b\b\b\b 44%\b\b\b\b 45%\b\b\b\b 46%\b\b\b\b 47%\b\b\b\b 48%\b\b\b\b 49%\b\b\b\b 50%\b\b\b\b 51%\b\b\b\b 52%\b\b\b\b 53%\b\b\b\b 54%\b\b\b\b 55%\b\b\b\b 56%\b\b\b\b 57%\b\b\b\b 58%\b\b\b\b 59%\b\b\b\b 60%\b\b\b\b 61%\b\b\b\b 62%\b\b\b\b 63%\b\b\b\b 64%\b\b\b\b 65%\b\b\b\b 66%\b\b\b\b 67%\b\b\b\b 68%\b\b\b\b 69%\b\b\b\b 70%\b\b\b\b 71%\b\b\b\b 72%\b\b\b\b 73%\b\b\b\b 74%\b\b\b\b 75%\b\b\b\b 76%\b\b\b\b 77%\b\b\b\b 78%\b\b\b\b 79%\b\b\b\b 80%\b\b\b\b 81%\b\b\b\b 82%\b\b\b\b 83%\b\b\b\b 84%\b\b\b\b 85%\b\b\b\b 86%\b\b\b\b 87%\b\b\b\b 88%\b\b\b\b 89%\b\b\b\b 90%\b\b\b\b 91%\b\b\b\b 92%\b\b\b\b 93%\b\b\b\b 94%\b\b\b\b 95%\b\b\b\b 96%\b\b\b\b 97%\b\b\b\b 98%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "\n",
            "Would you like to replace the existing file /content/el_7aga/LoadMat.py\n",
            "   199 bytes, modified on 2022-06-02 09:37\n",
            "with a new one\n",
            "   199 bytes, modified on 2022-06-02 09:37\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit y\n",
            "\n",
            "Extracting  /content/el_7aga/LoadMat.py                                  \b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "\n",
        "dataset = scipy.io.loadmat('/content/el_7aga/DLCV_data.mat')\n",
        "Labels = list(dataset.keys())[3:13]\n",
        "dataset = dict(list(dataset.items())[:13])\n",
        "print(\"Labels: \", Labels)\n",
        "print(\"Class Dimensions: \", dataset[Labels[0]].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UImz6VLaEsNq",
        "outputId": "c85aa047-b10b-4aa7-c9e8-5ee44568baf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels:  ['ANIMAL', 'BLOOD', 'COOKING', 'CRYING', 'DANCING', 'DINING_GROUP', 'DRIVING', 'FIGHTING', 'FOOTBALL', 'GROUP_FIGHTING']\n",
            "Class Dimensions:  (400, 480, 640, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training hyper parameters"
      ],
      "metadata": {
        "id": "lvzxGNZFuVHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5\n",
        "n_epochs = 20\n",
        "learning_rate=0.0005"
      ],
      "metadata": {
        "id": "6DjW7A9QubWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loading, preprocessing, and batching the data"
      ],
      "metadata": {
        "id": "LZnzI-1TuyMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(data_holder,data,start,end, label):\n",
        "  datapoints_count = 0\n",
        "  for i in range(start, end):\n",
        "    datapoints_count+=1\n",
        "    image_path = data[i]\n",
        "    # reading the image\n",
        "    img = PIL.Image.fromarray(image_path, 'RGB')\n",
        "    img = np.array(img) \n",
        "    img = img[:, :, ::-1].copy() \n",
        "    img = cv2.resize(img, (50,50), interpolation = cv2.INTER_AREA)\n",
        "    # converting the type of pixel to float 32\n",
        "    img = img.astype('float32')\n",
        "    # normalizing the pixel values\n",
        "    img /= 255.0\n",
        "    # appending the image into the list\n",
        "    data_holder.append((img,label))\n",
        "  cv2.waitKey(0)\n",
        "  cv2.destroyAllWindows()\n",
        "  return (data_holder)"
      ],
      "metadata": {
        "id": "ExgW7QhGx0Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_data(data, batch_size):\n",
        "  data = torch.from_numpy(data)\n",
        "  num_batches = math.ceil(data.size()[0]/batch_size)\n",
        "  data = data.chunk(num_batches)\n",
        "  return data"
      ],
      "metadata": {
        "id": "F46xTtQqyr-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_images([],dataset[Labels[0]], 0, 400, 0)\n",
        "data = load_images(data,dataset[Labels[1]], 0, 400, 1)\n",
        "data = load_images(data,dataset[Labels[2]], 0, 400, 2)\n",
        "data = load_images(data,dataset[Labels[3]], 0, 400, 3)\n",
        "data = load_images(data,dataset[Labels[4]], 0, 400, 4)\n",
        "data = load_images(data,dataset[Labels[5]], 0, 400, 5)\n",
        "data = load_images(data,dataset[Labels[6]], 0, 400, 6)\n",
        "data = load_images(data,dataset[Labels[7]], 0, 400, 7)\n",
        "data = load_images(data,dataset[Labels[8]], 0, 400, 8)\n",
        "data = load_images(data,dataset[Labels[9]], 0, 400, 9)\n",
        "random.shuffle(data)\n",
        "data_features = np.array([data[i][0] for i in range(len(data))])\n",
        "data_labels = np.array([data[i][1] for i in range(len(data))])\n",
        "development_features, test_features, development_labesl, test_labesl = train_test_split(data_features, data_labels, test_size = 0.1)\n",
        "# create validation set\n"
      ],
      "metadata": {
        "id": "54ta2e4GzQKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model architecture, forward pass, and learning process"
      ],
      "metadata": {
        "id": "kpEzdrTUvJr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(Module):   \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.cnn_layers = Sequential(\n",
        "            # Defining a 2D convolution layer\n",
        "            Conv2d(in_channels=3, out_channels=48, kernel_size=3, stride=1),\n",
        "            BatchNorm2d(48),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2), 
        "            # Defining 2nd 2D convolution layer\n",
        "            Conv2d(48, 96, kernel_size=5, stride=1),\n",
        "            BatchNorm2d(96),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2), 
        "            # Defining 3rd 2D convolution layer\n",
        "            Conv2d(96, 144, kernel_size=7, stride=1),\n",
        "            BatchNorm2d(144),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2), 
        "        )\n",
        "\n",
        "        self.linear_layers = Sequential(\n",
        "            Linear(2 * 2 * 144, 72),\n",
        "            Sigmoid(),\n",
        "            Linear(72, 10)\n",
        "\n",
        "        )\n",
        "    \n",
        "        self.init_bias()  # initialize bias\n",
        "\n",
        "    def init_bias(self):\n",
        "            for layer in self.cnn_layers:\n",
        "                if isinstance(layer, Conv2d):\n",
        "                    init.normal_(layer.weight, mean=0, std=0.01)\n",
        "                    init.constant_(layer.bias, 0)\n",
        "                    # init.constant_(layer.weight, 0.1)\n",
        "                elif isinstance(layer, Linear):\n",
        "                    init.xavier_uniform(layer.weight)\n",
        "                    # init.constant_(layer.weight, 0.1)\n",
        "                    layer.bias.data.fill_(0.01)    \n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        return x\n",
        "\n",
        "# defining the model\n",
        "model = Net()\n",
        "# defining the optimizer\n",
        "optimizer = Adam(model.parameters(), learning_rate)\n",
        "# defining the loss function\n",
        "criterion = CrossEntropyLoss()\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    \n",
        "    for i in range(len(train_x)):\n",
        "      training_loss = 0\n",
        "      validation_loss = 0\n",
        "      # getting the training set\n",
        "      x_train, y_train = Variable(train_x[i]), Variable(train_y[i])\n",
        "      \n",
        "      # converting the data into GPU format\n",
        "      if torch.cuda.is_available():\n",
        "          x_train = x_train.cuda()\n",
        "          y_train = y_train.cuda()\n",
        "         \n",
        "\n",
        "      # clearing the Gradients of the model parameters\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # prediction for training and validation set\n",
        "      output_train = model(x_train)\n",
        "      # print(output_train.shape, y_train.shape)\n",
        "\n",
        "      # computing the training and validation loss\n",
        "      loss_train = criterion(output_train, y_train)\n",
        "\n",
        "      # computing the updated weights of all the model parameters\n",
        "      loss_train.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        model.eval()\n",
        "        for j in range(len(val_x)):\n",
        "          # getting the validation set\n",
        "          x_val, y_val = Variable(val_x[j]), Variable(val_y[j])\n",
        "          if torch.cuda.is_available():\n",
        "            x_val = x_val.cuda()\n",
        "            y_val = y_val.cuda()\n",
        "          output_val = model(x_val) \n",
        "          loss_val = criterion(output_val, y_val) \n",
        "          validation_loss += loss_val\n",
        "\n",
        "    if epoch%2 == 0 or epoch == 80:\n",
        "        avg_val_loss = validation_loss / len(val_x)\n",
        "        # printing the validation loss\n",
        "        print('Epoch : ',epoch+1, '\\t', 'avg val_loss per batch:', avg_val_loss, 'total validation loss', validation_loss)\n",
        "\n",
        "def predict(features,labels):\n",
        "  features = features.cuda()\n",
        "  with torch.no_grad():\n",
        "    output = model(features)\n",
        "    \n",
        "  softmax = torch.exp(output).cpu()\n",
        "  prob = list(softmax.numpy())\n",
        "  predictions = np.argmax(prob, axis=1)\n",
        "  # print(predictions,labels )\n",
        "\n",
        "  # accuracy on training set\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "SJ5eE-tUq2l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model training"
      ],
      "metadata": {
        "id": "si9CvUl4wdq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = test_features.reshape(400,3,50,50)\n",
        "test_features = torch.from_numpy(test_features)"
      ],
      "metadata": {
        "id": "V4O_ozd8ZpV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### k-Fold Cross-Validation 1"
      ],
      "metadata": {
        "id": "h2eQ837TTRvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x1 = development_features[685:]\n",
        "val_x1 = development_features[0:684]\n",
        "train_y1 = development_labesl[685:]\n",
        "val_y1 = development_labesl[0:684]\n",
        "(train_x1.shape, train_y1.shape), (val_x1.shape, val_y1.shape), (type(train_x1))\n",
        "train_x = train_x1.reshape(2915, 3, 50, 50)\n",
        "train_x  = batch_data(train_x, batch_size)\n",
        "# converting the target into torch format\n",
        "train_y = train_y1.astype(int);\n",
        "train_y = batch_data(train_y, batch_size)\n",
        "# shape of training data\n",
        "type(train_x[0]), train_x[0].size()\n",
        "# converting validation images into torch format\n",
        "val_x = val_x1.reshape(684, 3, 50, 50)\n",
        "val_x  = batch_data(val_x, batch_size)\n",
        "\n",
        "# converting the target into torch formaty\n",
        "val_y = val_y1.astype(int);\n",
        "val_y = batch_data(val_y, batch_size)"
      ],
      "metadata": {
        "id": "Q8mHT4pIaDgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "id": "YJy1Ks7L2a1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(test_labesl, predict(test_features, test_labesl))"
      ],
      "metadata": {
        "id": "Xwh-b_jeV09e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### k-Fold Cross-Validation 2"
      ],
      "metadata": {
        "id": "_9zL3B-QWpse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x2 = np.concatenate((development_features[0:684],development_features[1369:]), axis = 0)\n",
        "train_y2 = np.concatenate((development_labesl[0:684],development_labesl[1369:]), axis = 0)\n",
        "val_x2 = development_features[685:1369]\n",
        "val_y2 = development_labesl[685:1369]\n",
        "(train_x2.shape, train_y2.shape), (val_x2.shape, val_y2.shape), (type(train_x1))\n",
        "train_x = train_x2.reshape(2915, 3, 50, 50)\n",
        "train_x  = batch_data(train_x, batch_size)\n",
        "# converting the target into torch format\n",
        "train_y = train_y2.astype(int);\n",
        "train_y = batch_data(train_y, batch_size)\n",
        "# shape of training data\n",
        "type(train_x[0]), train_x[0].size()\n",
        "# converting validation images into torch format\n",
        "val_x = val_x2.reshape(684, 3, 50, 50)\n",
        "val_x  = batch_data(val_x, batch_size)\n",
        "\n",
        "# converting the target into torch format\n",
        "val_y = val_y2.astype(int);\n",
        "val_y = batch_data(val_y, batch_size)"
      ],
      "metadata": {
        "id": "p3BFE3eYamJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "id": "PlMrxw0DWmpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(test_labesl, predict(test_features, test_labesl))"
      ],
      "metadata": {
        "id": "VbUUowPkWn2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### k-Fold Cross-Validation 3"
      ],
      "metadata": {
        "id": "7VProz70XVrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x3 = np.concatenate((development_features[0:1369],development_features[2054:]), axis = 0)\n",
        "train_y3 = np.concatenate((development_labesl[0:1369],development_labesl[2054:]), axis = 0)\n",
        "val_x3 = development_features[1370:2054]\n",
        "val_y3 = development_labesl[1370:2054]\n",
        "(train_x3.shape, train_y3.shape), (val_x3.shape, val_y3.shape), (type(train_x1))\n",
        "train_x = train_x3.reshape(2915, 3, 50, 50)\n",
        "train_x  = batch_data(train_x, batch_size)\n",
        "# converting the target into torch format\n",
        "train_y = train_y3.astype(int);\n",
        "train_y = batch_data(train_y, batch_size)\n",
        "# shape of training data\n",
        "type(train_x[0]), train_x[0].size()\n",
        "# converting validation images into torch format\n",
        "val_x = val_x3.reshape(684, 3, 50, 50)\n",
        "val_x  = batch_data(val_x, batch_size)\n",
        "\n",
        "# converting the target into torch format\n",
        "val_y = val_y3.astype(int);\n",
        "val_y = batch_data(val_y, batch_size)"
      ],
      "metadata": {
        "id": "_t7NiBBdX2bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "id": "8LA0W2QTXPLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(test_features, test_labesl)"
      ],
      "metadata": {
        "id": "aLANmw-tXRSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### k-Fold Cross-Validation 4"
      ],
      "metadata": {
        "id": "YIl0byp1Xq10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x4 = np.concatenate((development_features[0:2057],development_features[2740:]), axis = 0)\n",
        "train_y4 = np.concatenate((development_labesl[0:2057],development_labesl[2740:]), axis = 0)\n",
        "val_x4 = development_features[2054:2738]\n",
        "val_y4 = development_labesl[2054:2738]\n",
        "(train_x4.shape, train_y4.shape), (val_x4.shape, val_y4.shape), (type(train_x1))\n",
        "train_x = train_x4.reshape(2917, 3, 50, 50)\n",
        "train_x  = batch_data(train_x, batch_size)\n",
        "# converting the target into torch format\n",
        "train_y = train_y4.astype(int);\n",
        "train_y = batch_data(train_y, batch_size)\n",
        "# shape of training data\n",
        "type(train_x[0]), train_x[0].size()\n",
        "# converting validation images into torch format\n",
        "val_x = val_x4.reshape(684, 3, 50, 50)\n",
        "val_x  = batch_data(val_x, batch_size)\n",
        "\n",
        "# converting the target into torch format\n",
        "val_y = val_y4.astype(int);\n",
        "val_y = batch_data(val_y, batch_size)"
      ],
      "metadata": {
        "id": "NJx5Kq6WcBlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "id": "egcGCFWMXuCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(test_features, test_labesl)"
      ],
      "metadata": {
        "id": "ELGi-RDHXwoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model predication\n"
      ],
      "metadata": {
        "id": "mEW3y2LqwsSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, val_x, train_y, val_y = train_test_split(development_features, development_labesl, test_size = 0.18)\n",
        "(train_x.shape, train_y.shape), (val_x.shape, val_y.shape), (type(train_x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61fAtEgxc02U",
        "outputId": "f592ff23-47dd-492c-b23d-5592bdd85389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(((2952, 50, 50, 3), (2952,)), ((648, 50, 50, 3), (648,)), numpy.ndarray)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_x.reshape(2952, 3, 50, 50)\n",
        "train_x  = batch_data(train_x, batch_size)\n",
        "# converting the target into torch format\n",
        "train_y = train_y.astype(int);\n",
        "train_y = batch_data(train_y, batch_size)\n",
        "# shape of training data\n",
        "type(train_x[0]), train_x[0].size()\n",
        "# converting validation images into torch format\n",
        "val_x = val_x.reshape(648, 3, 50, 50)\n",
        "val_x  = batch_data(val_x, batch_size)\n",
        "\n",
        "# converting the target into torch format\n",
        "val_y = val_y.astype(int);\n",
        "val_y = batch_data(val_y, batch_size)"
      ],
      "metadata": {
        "id": "-OWfwl9ic9bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "itr_pred = []\n",
        "for epoch in range(n_epochs):\n",
        "    train(epoch)\n",
        "    itr_pred.append(accuracy_score(test_labesl, predict(test_features, test_labesl)))"
      ],
      "metadata": {
        "id": "jBPkB1lXZq0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "accuracy per iteration"
      ],
      "metadata": {
        "id": "RgRZFraGZRET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(0,60) \n",
        "print(x.shape)\n",
        "y = np.array(itr_pred)\n",
        "plt.title(\"Accuracy per iteration\") \n",
        "plt.xlabel(\"iteration\") \n",
        "plt.ylabel(\"accuracy\") \n",
        "plt.plot(x,y) \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "om61Nr4yRfgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "confusion matrix"
      ],
      "metadata": {
        "id": "j5LXmHcLZUw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict(test_features, test_labesl)\n",
        "cm = confusion_matrix(test_labesl, predictions)\n",
        "classes_acc = cm.diagonal()/cm.sum(axis=1)\n",
        "cm = np.concatenate((cm, classes_acc.reshape(-1,1)), axis=1)\n",
        "Labels = list(dataset.keys())[3:13]\n",
        "Labels.append('Accuracy')\n",
        "yticks = Labels[:10]\n",
        "xticks = Labels\n",
        "xticks\n",
        "plt.yticks(rotation=0) \n",
        "ax = sns.heatmap(cm,linewidth=0,annot=True, yticklabels=yticks,xticklabels=xticks)\n",
        "ax.set_title('Confusion Matrix')\n",
        "# This sets the yticks \"upright\" with 0, as opposed to sideways with 90.\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "REapmfprv0-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "overall model accuracy"
      ],
      "metadata": {
        "id": "qfcijc5zZXZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(test_labesl, predictions)"
      ],
      "metadata": {
        "id": "bAXh6iVo7YzQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
